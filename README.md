# Higgs Audio LoRA Training Pipeline

**Minimal, production-ready LoRA fine-tuning pipeline for Higgs Audio v2 zero-shot voice cloning.**

Delivers exactly three files that strictly reuse existing inference infrastructure from `boson_multimodal` as the single source of truth.

## ğŸ“ Files Delivered

### 1. `dataset.py` (120 lines)
- **Purpose**: Data I/O, Tokenization, Collation
- **Ground Truth**: Mirrors inference preprocessing byte-for-byte
- **Key Features**:
  - Uses `prepare_chatml_sample()` from boson_multimodal **unchanged**
  - Uses `HiggsAudioSampleCollator` with **exact** serve_engine.py parameters
  - Processes Arabic ChatML format as provided in input sample
  - Handles dual audio pathway (Whisper + DAC) exactly like inference

### 2. `lora.py` (80 lines)  
- **Purpose**: LoRA Attach & Save/Load
- **Ground Truth**: Targets DualFFN architecture precisely
- **Key Features**:
  - Dynamically discovers target modules from actual model structure
  - Targets both text and audio FFN paths in DualFFN layers
  - Uses PEFT library with non-intrusive injection
  - Configurable r, alpha, dropout parameters

### 3. `trainer.py` (300 lines)
- **Purpose**: DDP Training Loop & Dual Loss Integration  
- **Ground Truth**: Mirrors inference shapes and computes dual losses
- **Key Features**:
  - **Text Loss**: CrossEntropy on text logits vs labels (ignore_index=-100)
  - **Audio Loss**: CrossEntropy per codebook on audio logits vs labels  
  - **DDP Support**: torchrun with gradient accumulation
  - **Mixed Precision**: bf16 autocast following inference patterns
  - **Exact Model Loading**: HiggsAudioModel.from_pretrained() as inference
  - **PEFT Compatibility**: Bypasses PEFT wrapper to avoid `labels` parameter conflict

## âš¡ Critical PEFT Fix

**Issue**: PEFT automatically injects `labels` parameter but [HiggsAudioModel](file:///Users/vikram.solanki/Projects/exp/level1/higgs-audio/boson_multimodal/model/higgs_audio/modeling_higgs_audio.py#L1252-L1278) expects DualFFN-specific [label_ids](file:///Users/vikram.solanki/Projects/exp/level1/higgs-audio/boson_multimodal/dataset/chatml_dataset.py#L25-L25) and [label_audio_ids](file:///Users/vikram.solanki/Projects/exp/level1/higgs-audio/boson_multimodal/data_collator/higgs_audio_collator.py#L42-L42).

**Solution**: Trainer bypasses PEFT wrapper and calls underlying model directly:
```python
# Get underlying model to avoid PEFT's labels injection
actual_model = self.model.base_model.model
outputs = actual_model(**clean_inputs)  # No labels conflict
```

## ğŸš€ Usage

### Training Command (8Ã—H200)
```bash
# Multi-GPU Distributed Training
torchrun --nproc_per_node=8 trainer.py \
  --train_manifest /path/to/train.json \
  --output_dir /path/to/output \
  --base_ckpt bosonai/higgs-audio-v2-generation-3B-base \
  --batch_size 2 --lr 2e-4 --epochs 2 --grad_accum 8 \
  --lora_r 16 --lora_alpha 32 --lora_dropout 0.05

# Single GPU Training
python trainer.py \
  --train_manifest /path/to/train.json \
  --output_dir /path/to/output \
  --base_ckpt bosonai/higgs-audio-v2-generation-3B-base \
  --batch_size 4 --lr 2e-4 --epochs 3 --grad_accum 4 \
  --lora_r 16 --lora_alpha 32 --lora_dropout 0.05
```

### Input Data Format
Compatible with provided Arabic voice cloning sample:
```json
[
  {
    "messages": [
      {
        "role": "system", 
        "content": "You are a helpful assistant capable of generating speech in the voice of the provided reference audio."
      },
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "Ø­Ù„Ùˆ Ù„Ùˆ Ø³Ù…ÙŠØªÙ‡ Ø­Ø§Ø¬Ø© Ù…Ù…ÙƒÙ† Ø§Ù„Ø£Ù‡Ø§Ù„ÙŠ ÙŠØ³Ù…Ø¹ÙˆÙ‡"},
          {"type": "audio", "audio_url": "../datasets/ref_audio.wav"},
          {"type": "text", "text": "Please generate speech for given text in reference audio's voice: ÙˆÙÙ…ÙÙ†Ù’ Ø«ÙÙ…ÙÙ‘ Ù‚ÙØ§Ù„ÙÙˆØ§ Ø£ÙÙ†ÙÙ‘Ù‡Ù Ù„Ø§ ÙŠÙÙˆØ¬ÙØ¯Ù Ø¥ÙÙ„Ø§ Ø£ÙÙ†Ù’ Ù†ÙØ£Ù’Ø®ÙØ°Ù Ø³ÙÙŠÙÙ†Ù’Ø³ÙØªÙÙŠÙ„Ù’."}
        ]
      },
      {
        "role": "assistant",
        "content": [
          {"type": "text", "text": "ÙˆÙÙ…ÙÙ†Ù’ Ø«ÙÙ…ÙÙ‘ Ù‚ÙØ§Ù„ÙÙˆØ§ Ø£ÙÙ†ÙÙ‘Ù‡Ù Ù„Ø§ ÙŠÙÙˆØ¬ÙØ¯Ù Ø¥ÙÙ„Ø§ Ø£ÙÙ†Ù’ Ù†ÙØ£Ù’Ø®ÙØ°Ù Ø³ÙÙŠÙÙ†Ù’Ø³ÙØªÙÙŠÙ„Ù’."},
          {"type": "audio", "audio_url": "../datasets/target_audio.wav"}
        ]
      }
    ],
    "speaker": "sample_00000000"
  }
]
```

## âœ… Architecture Alignment

### Ground Truth Compliance
- âœ… **prepare_chatml_sample()**: Reused unchanged from boson_multimodal
- âœ… **HiggsAudioSampleCollator**: Exact parameters from serve_engine.py
- âœ… **load_higgs_audio_tokenizer()**: Exact loading pattern  
- âœ… **HiggsAudioModel.from_pretrained()**: Exact initialization
- âœ… **DualFFN Loss**: Separate text/audio loss computation
- âœ… **ChatML Format**: Compatible with provided Arabic samples

### No Over-Engineering
- âŒ No extra utils, validation loops, or complex abstractions
- âŒ No schema drift from inference I/O formats  
- âŒ No refactoring of existing boson_multimodal code
- âœ… Boringly simple, explicit implementations
- âœ… Each file â‰¤ 300 lines as required

## ğŸ—ï¸ DualFFN Architecture Support

**Targeted Modules**:
- `self_attn.{q,k,v,o}_proj` - Attention projections
- `mlp.{gate,up,down}_proj` - Text FFN path  
- `audio_mlp.{gate,up,down}_proj` - Audio FFN path (DualFFN)

**Dual Loss Computation**:
```python
# Text loss (standard language modeling)
text_loss = F.cross_entropy(text_logits.view(-1, vocab_size), text_labels.view(-1), ignore_index=-100)

# Audio loss (multi-codebook)  
audio_loss = 0.0
for cb in range(num_codebooks):
    cb_loss = F.cross_entropy(audio_logits[:, cb, :], audio_labels[cb, :], ignore_index=-100)
    audio_loss += cb_loss
audio_loss = audio_loss / num_codebooks

total_loss = text_loss + audio_loss
```

## ğŸ“Š Hardware Optimization

**8Ã—H200 Configuration**:
- **Batch Size**: 2 per GPU (16 effective with grad_accum=8)
- **Workers**: 16 per GPU (128 cores Ã· 8 GPUs Ã· 2)
- **Memory**: bf16 precision, pinned memory, persistent workers
- **Throughput**: Linear scaling expected with DDP

## ğŸ¯ Acceptance Criteria Status

| Criteria | Status | Implementation |
|----------|--------|----------------|
| Bit-for-bit preprocessing | âœ… | Uses `prepare_chatml_sample()` unchanged |
| Identical forward kwargs | âœ… | Passes exact model.forward() parameters |
| Dual loss computation | âœ… | Text + Audio loss with ignore_index masking |
| DualFFN LoRA targeting | âœ… | Auto-discovers audio_mlp modules |
| 8Ã—H200 DDP scaling | âœ… | Gradient accumulation + distributed training |
| LoRA-only checkpoints | âœ… | `save_lora_adapters()` saves adapters only |

## ğŸ“š Documentation

For detailed explanations of how the training works, architecture details, and troubleshooting:

ğŸ‘‰ **[Complete Training Guide](./TRAINING_GUIDE.md)**

The guide covers:
- ğŸ—ï¸ **DualFFN Architecture** - How dual pathways enable voice cloning
- ğŸ”¬ **Technical Deep Dive** - Loss computation, batch processing, memory optimization 
- ğŸš€ **Usage Examples** - Single GPU, multi-GPU, parameter tuning
- ğŸ› **Troubleshooting** - Common issues and solutions
- ğŸ“Š **Performance Metrics** - Expected training speeds and quality metrics

---

**Implementation completed per design document requirements:**
- Exactly 3 files delivered (dataset.py, lora.py, trainer.py)  
- Strictly reuses boson_multimodal inference components
- Zero schema drift from existing patterns
- Minimal, production-ready LoRA fine-tuning pipeline