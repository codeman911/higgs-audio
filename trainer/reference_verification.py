"""
Reference Audio Conditioning Pipeline Verification.

Validates that the training pipeline exactly matches the patterns from arb_inference.py:\n- Dual audio processing (Whisper + DAC)\n- ChatML message structure alignment\n- Collator configuration consistency\n- Audio conditioning pipeline verification\n- Sample construction validation\n\nEnsures perfect alignment between training and inference for optimal voice cloning.\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom loguru import logger\nimport json\nfrom pathlib import Path\n\n\nclass ReferenceConditioningVerifier:\n    \"\"\"\n    Comprehensive verifier for reference audio conditioning alignment.\n    \n    Validates that training pipeline matches arb_inference.py patterns exactly.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the reference conditioning verifier.\"\"\"\n        self.verification_results = {}\n        self.alignment_issues = []\n    \n    def verify_complete_pipeline_alignment(\n        self,\n        trainer,\n        sample_batch=None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Comprehensive verification of pipeline alignment with arb_inference.py.\n        \n        Args:\n            trainer: HiggsAudioTrainer instance\n            sample_batch: Optional sample batch for detailed validation\n            \n        Returns:\n            Dictionary containing comprehensive verification results\n        \"\"\"\n        logger.info(\"\ud83d\udd0d Comprehensive Pipeline Alignment Verification\")\n        logger.info(\"=\" * 60)\n        \n        verification_results = {\n            'timestamp': str(torch.datetime.now() if hasattr(torch, 'datetime') else 'unknown'),\n            'alignment_score': 0.0,\n            'checks': {},\n            'issues': [],\n            'recommendations': []\n        }\n        \n        # 1. Verify collator configuration alignment\n        collator_score = self._verify_collator_alignment(trainer, verification_results)\n        \n        # 2. Verify dual audio processing pipeline\n        audio_processing_score = self._verify_audio_processing_pipeline(trainer, verification_results)\n        \n        # 3. Verify ChatML message structure\n        chatml_score = self._verify_chatml_structure(trainer, verification_results)\n        \n        # 4. Verify model configuration alignment\n        model_config_score = self._verify_model_configuration(trainer, verification_results)\n        \n        # 5. Verify sample batch processing (if provided)\n        batch_processing_score = 1.0\n        if sample_batch is not None:\n            batch_processing_score = self._verify_batch_processing(trainer, sample_batch, verification_results)\n        \n        # Calculate overall alignment score\n        scores = [collator_score, audio_processing_score, chatml_score, model_config_score, batch_processing_score]\n        verification_results['alignment_score'] = np.mean(scores)\n        \n        # Generate recommendations\n        self._generate_alignment_recommendations(verification_results)\n        \n        # Log results\n        self._log_verification_results(verification_results)\n        \n        return verification_results\n    \n    def _verify_collator_alignment(self, trainer, results: Dict[str, Any]) -> float:\n        \"\"\"Verify collator configuration matches arb_inference.py exactly.\"\"\"\n        logger.info(\"\ud83c\udfb5 Verifying Collator Alignment with arb_inference.py\")\n        \n        collator = trainer.collator\n        issues = []\n        score = 1.0\n        \n        # Critical arb_inference.py collator settings\n        expected_settings = {\n            'return_audio_in_tokens': False,  # CRITICAL: serve_engine.py uses False\n            'round_to': 1,                    # CRITICAL: serve_engine.py uses fixed round_to=1\n            'use_delay_pattern': True,        # Should match model config\n            'encode_whisper_embed': True,     # Should be enabled for voice cloning\n        }\n        \n        alignment_checks = {}\n        for setting, expected_value in expected_settings.items():\n            actual_value = getattr(collator, setting, None)\n            is_aligned = actual_value == expected_value\n            alignment_checks[setting] = {\n                'expected': expected_value,\n                'actual': actual_value,\n                'aligned': is_aligned\n            }\n            \n            if not is_aligned:\n                issues.append(f\"Collator {setting}: expected {expected_value}, got {actual_value}\")\n                score -= 0.2\n        \n        # Verify Whisper processor availability\n        whisper_available = collator.whisper_processor is not None\n        alignment_checks['whisper_processor_available'] = {\n            'expected': True,\n            'actual': whisper_available,\n            'aligned': whisper_available\n        }\n        \n        if not whisper_available:\n            issues.append(\"Whisper processor not available - will use DAC-only mode\")\n            score -= 0.1\n        \n        logger.info(f\"   Collator alignment score: {score:.2f}\")\n        \n        results['checks']['collator_alignment'] = {\n            'score': score,\n            'settings': alignment_checks,\n            'issues': issues\n        }\n        \n        return score\n    \n    def _verify_audio_processing_pipeline(self, trainer, results: Dict[str, Any]) -> float:\n        \"\"\"Verify dual audio processing pipeline matches arb_inference.py.\"\"\"\n        logger.info(\"\ud83c\udfac Verifying Dual Audio Processing Pipeline\")\n        \n        issues = []\n        score = 1.0\n        \n        # Check audio tokenizer availability\n        audio_tokenizer_available = trainer.audio_tokenizer is not None\n        if not audio_tokenizer_available:\n            issues.append(\"Audio tokenizer not available\")\n            score -= 0.5\n        \n        # Check dataset dual processing capability\n        dataset = trainer.train_dataset\n        dataset_has_dual_processing = hasattr(dataset, '_process_reference_audio_dual_pathway')\n        if not dataset_has_dual_processing:\n            issues.append(\"Dataset missing dual audio processing method\")\n            score -= 0.3\n        \n        # Check force_whisper_embed setting\n        force_whisper = getattr(dataset, 'force_whisper_embed', False)\n        if not force_whisper:\n            issues.append(\"Dataset not configured to force Whisper embedding\")\n            score -= 0.2\n        \n        logger.info(f\"   Audio processing pipeline score: {score:.2f}\")\n        \n        results['checks']['audio_processing'] = {\n            'score': score,\n            'audio_tokenizer_available': audio_tokenizer_available,\n            'dataset_dual_processing': dataset_has_dual_processing,\n            'force_whisper_embed': force_whisper,\n            'issues': issues\n        }\n        \n        return score\n    \n    def _verify_chatml_structure(self, trainer, results: Dict[str, Any]) -> float:\n        \"\"\"Verify ChatML structure matches arb_inference.py patterns.\"\"\"\n        logger.info(\"\ud83d\udcdd Verifying ChatML Structure Alignment\")\n        \n        issues = []\n        score = 1.0\n        \n        # Check dataset ChatML implementation\n        dataset = trainer.train_dataset\n        has_inference_aligned_messages = hasattr(dataset, '_create_inference_aligned_messages')\n        has_component_extraction = hasattr(dataset, '_extract_sample_components')\n        \n        if not has_inference_aligned_messages:\n            issues.append(\"Dataset missing inference-aligned message creation\")\n            score -= 0.3\n        \n        if not has_component_extraction:\n            issues.append(\"Dataset missing component extraction (arb_inference.py pattern)\")\n            score -= 0.3\n        \n        # Check if dataset uses prepare_chatml_sample correctly\n        sample_creation_method = getattr(dataset, '__getitem__', None)\n        if sample_creation_method is None:\n            issues.append(\"Dataset missing __getitem__ method\")\n            score -= 0.4\n        \n        logger.info(f\"   ChatML structure score: {score:.2f}\")\n        \n        results['checks']['chatml_structure'] = {\n            'score': score,\n            'inference_aligned_messages': has_inference_aligned_messages,\n            'component_extraction': has_component_extraction,\n            'issues': issues\n        }\n        \n        return score\n    \n    def _verify_model_configuration(self, trainer, results: Dict[str, Any]) -> float:\n        \"\"\"Verify model configuration matches arb_inference.py patterns.\"\"\"\n        logger.info(\"\ud83d\udee0\ufe0f Verifying Model Configuration\")\n        \n        issues = []\n        score = 1.0\n        \n        model_config = trainer.model_config\n        \n        # Check critical configuration parameters\n        expected_configs = {\n            'use_delay_pattern': True,\n            'audio_num_codebooks': 12,  # Standard DAC configuration\n        }\n        \n        config_checks = {}\n        for config_name, expected_value in expected_configs.items():\n            actual_value = getattr(model_config, config_name, None)\n            is_correct = actual_value == expected_value\n            config_checks[config_name] = {\n                'expected': expected_value,\n                'actual': actual_value,\n                'correct': is_correct\n            }\n            \n            if not is_correct:\n                issues.append(f\"Model config {config_name}: expected {expected_value}, got {actual_value}\")\n                score -= 0.2\n        \n        # Check if model is in training mode\n        model_in_training = trainer.model.training\n        if not model_in_training:\n            issues.append(\"Model not in training mode\")\n            score -= 0.1\n        \n        # Check LoRA configuration\n        has_lora = hasattr(trainer.model, 'peft_config')\n        if not has_lora:\n            issues.append(\"LoRA not properly configured\")\n            score -= 0.3\n        \n        logger.info(f\"   Model configuration score: {score:.2f}\")\n        \n        results['checks']['model_configuration'] = {\n            'score': score,\n            'config_checks': config_checks,\n            'model_training_mode': model_in_training,\n            'lora_configured': has_lora,\n            'issues': issues\n        }\n        \n        return score\n    \n    def _verify_batch_processing(self, trainer, sample_batch, results: Dict[str, Any]) -> float:\n        \"\"\"Verify batch processing matches expected patterns.\"\"\"\n        logger.info(\"\ud83d\udce6 Verifying Sample Batch Processing\")\n        \n        issues = []\n        score = 1.0\n        \n        # Check batch structure\n        required_fields = ['input_ids', 'attention_mask']\n        optional_fields = ['audio_features', 'audio_out_ids', 'label_ids', 'label_audio_ids']\n        \n        batch_structure = {}\n        for field in required_fields:\n            has_field = hasattr(sample_batch, field)\n            batch_structure[field] = {\n                'present': has_field,\n                'required': True\n            }\n            if not has_field:\n                issues.append(f\"Missing required batch field: {field}\")\n                score -= 0.3\n        \n        for field in optional_fields:\n            has_field = hasattr(sample_batch, field)\n            batch_structure[field] = {\n                'present': has_field,\n                'required': False\n            }\n        \n        # Check audio features if present\n        if hasattr(sample_batch, 'audio_features') and sample_batch.audio_features is not None:\n            audio_features_shape = sample_batch.audio_features.shape\n            logger.info(f\"   Audio features shape: {audio_features_shape}\")\n            \n            # Verify Whisper feature dimensions\n            if len(audio_features_shape) != 3:\n                issues.append(f\"Unexpected audio features shape: {audio_features_shape}\")\n                score -= 0.2\n        \n        # Check audio output IDs if present\n        if hasattr(sample_batch, 'audio_out_ids') and sample_batch.audio_out_ids is not None:\n            audio_out_shape = sample_batch.audio_out_ids.shape\n            logger.info(f\"   Audio output IDs shape: {audio_out_shape}\")\n            \n            # Verify multi-codebook structure\n            if len(audio_out_shape) != 2 or audio_out_shape[0] != 12:\n                issues.append(f\"Unexpected audio output IDs shape: {audio_out_shape} (expected [12, seq_len])\")\n                score -= 0.2\n        \n        logger.info(f\"   Batch processing score: {score:.2f}\")\n        \n        results['checks']['batch_processing'] = {\n            'score': score,\n            'batch_structure': batch_structure,\n            'issues': issues\n        }\n        \n        return score\n    \n    def _generate_alignment_recommendations(self, results: Dict[str, Any]):\n        \"\"\"Generate recommendations for improving alignment.\"\"\"\n        recommendations = []\n        \n        # Analyze issues and generate specific recommendations\n        all_issues = []\n        for check_name, check_data in results['checks'].items():\n            all_issues.extend(check_data.get('issues', []))\n        \n        # Collator recommendations\n        collator_issues = [issue for issue in all_issues if 'Collator' in issue]\n        if collator_issues:\n            recommendations.append(\n                \"Fix collator configuration to match serve_engine.py exactly: \"\n                \"return_audio_in_tokens=False, round_to=1\"\n            )\n        \n        # Audio processing recommendations\n        audio_issues = [issue for issue in all_issues if 'Audio' in issue or 'audio' in issue]\n        if audio_issues:\n            recommendations.append(\n                \"Ensure dual audio processing pipeline is implemented: \"\n                \"Whisper + DAC conditioning following arb_inference.py patterns\"\n            )\n        \n        # LoRA recommendations\n        lora_issues = [issue for issue in all_issues if 'LoRA' in issue or 'lora' in issue]\n        if lora_issues:\n            recommendations.append(\n                \"Configure LoRA properly targeting lm_head and audio_head modules\"\n            )\n        \n        # Overall alignment recommendation\n        if results['alignment_score'] < 0.9:\n            recommendations.append(\n                \"Review arb_inference.py implementation and ensure training pipeline \"\n                \"follows the exact same patterns for optimal voice cloning performance\"\n            )\n        \n        results['recommendations'] = recommendations\n    \n    def _log_verification_results(self, results: Dict[str, Any]):\n        \"\"\"Log comprehensive verification results.\"\"\"\n        alignment_score = results['alignment_score']\n        \n        logger.info(f\"\ud83c\udfc6 Overall Alignment Score: {alignment_score:.2%}\")\n        \n        if alignment_score >= 0.95:\n            logger.info(\"\u2705 EXCELLENT: Training pipeline perfectly aligned with arb_inference.py\")\n        elif alignment_score >= 0.85:\n            logger.info(\"\u2705 GOOD: Training pipeline well aligned with minor issues\")\n        elif alignment_score >= 0.70:\n            logger.warning(\"\u26a0\ufe0f FAIR: Training pipeline has alignment issues that should be addressed\")\n        else:\n            logger.error(\"\u274c POOR: Training pipeline has significant alignment issues\")\n        \n        # Log individual check scores\n        logger.info(\"\n\ud83d\udcca Individual Check Scores:\")\n        for check_name, check_data in results['checks'].items():\n            score = check_data['score']\n            status = \"\u2705\" if score >= 0.9 else \"\u26a0\ufe0f\" if score >= 0.7 else \"\u274c\"\n            logger.info(f\"   {status} {check_name}: {score:.2%}\")\n        \n        # Log recommendations\n        if results['recommendations']:\n            logger.info(\"\n\ud83d\udca1 Recommendations:\")\n            for i, rec in enumerate(results['recommendations'], 1):\n                logger.info(f\"   {i}. {rec}\")\n        \n        logger.info(\"=\" * 60)\n    \n    def save_verification_report(self, results: Dict[str, Any], output_path: str):\n        \"\"\"Save detailed verification report.\"\"\"\n        try:\n            with open(output_path, 'w') as f:\n                json.dump(results, f, indent=2, default=str)\n            \n            logger.info(f\"\ud83d\udcbe Verification report saved: {output_path}\")\n        except Exception as e:\n            logger.error(f\"\u274c Failed to save verification report: {e}\")\n\n\n# Global reference conditioning verifier instance\nreference_verifier = ReferenceConditioningVerifier()\n